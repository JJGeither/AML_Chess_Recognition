{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from zipfile import ZipFile\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, ConfusionMatrixDisplay, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from skimage.feature import hog\n",
    "from skimage.io import imread, imsave\n",
    "from skimage import color, exposure\n",
    "from skimage.transform import rescale\n",
    "from skimage.util import view_as_blocks\n",
    "from multiprocessing import Pool\n",
    "from skimage.color import rgb2gray\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import multiprocess as mp\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from multiprocessingNotebook import runt\n",
    "\"\"\"\n",
    "0 = all messages are logged (default behavior)\n",
    "1 = INFO messages are not printed\n",
    "2 = INFO and WARNING messages are not printed\n",
    "3 = INFO, WARNING, and ERROR messages are not printed \"\"\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from keras import callbacks\n",
    "\n",
    "PICKLE_PATH = 'test_data.pickle'\n",
    "\n",
    "NUM_TRAINING_IMAGES = 5\n",
    "NUM_TESTING_IMAGES = 5\n",
    "\n",
    "NUM_SPACES_PER_BOARD = 2\n",
    "PICTURE_DIMENSIONS = 400\n",
    "block_width = PICTURE_DIMENSIONS // 8\n",
    "block_height = PICTURE_DIMENSIONS // 8\n",
    "num_channels = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in train:  80000\n",
      "Files in test:  20000\n"
     ]
    }
   ],
   "source": [
    "from zipfile import ZipFile\n",
    "\n",
    "def getFilePaths(filePath):\n",
    "    # opens the chess boards.zip file into the chessBoards object\n",
    "    chessBoards = ZipFile(filePath, 'r')\n",
    "\n",
    "    test = []\n",
    "    train = []\n",
    "\n",
    "    # extract the file names from the train and test folders in the zip archive\n",
    "    for file in chessBoards.namelist():\n",
    "        # fills the file names from within the dataset subfolder\n",
    "        if file[:4] == 'test':\n",
    "            test.append(file)\n",
    "        if file[:5] == 'train':\n",
    "            train.append(file)\n",
    "\n",
    "    return train, test\n",
    "\n",
    "zipFilePath = \"../chess boards.zip\"\n",
    "\n",
    "trainPaths, testPaths = getFilePaths(zipFilePath)\n",
    "print(\"Files in train: \", len(trainPaths))\n",
    "print(\"Files in test: \", len(testPaths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running...\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "# Parallel Loads Board Images for Training Purposes\n",
    "def HogTransform(img):\n",
    "    first_image_gray = color.rgb2gray(img)\n",
    "\n",
    "    fd, hog_image = hog(\n",
    "        first_image_gray,\n",
    "        orientations=8,\n",
    "        pixels_per_cell=(8, 8),\n",
    "        cells_per_block=(1, 1),\n",
    "        visualize=True,\n",
    "        block_norm='L2-Hys',\n",
    "        feature_vector=True\n",
    "    )\n",
    "\n",
    "    hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10))\n",
    "    hog_image_uint8 = (hog_image_rescaled * 255).astype(np.uint8)\n",
    "    return hog_image_uint8\n",
    "\n",
    "def read_and_store_modified_chess_images():\n",
    "    return\n",
    "\n",
    "def fen_from_position(position, fen_string):\n",
    "    rows = position // 8\n",
    "    cols = position % 8\n",
    "\n",
    "    character = fen_string[rows * 8 + cols]\n",
    "\n",
    "    if character.isdigit():\n",
    "        return ' ' * int(character)\n",
    "    else:\n",
    "        return character\n",
    "\n",
    "def fen_from_filename(filename):\n",
    "    parts = filename.split('/')\n",
    "    fen_part = parts[-1].split('.')[0]\n",
    "\n",
    "    fen_string = ''.join([' ' * int(char) if char.isdigit() else char for char in fen_part])\n",
    "    fen_string = fen_string.replace('-', '')\n",
    "\n",
    "    return fen_string\n",
    "\n",
    "src = zipFilePath\n",
    "random.shuffle(trainPaths)\n",
    "\n",
    "print(\"Running...\")\n",
    "lock = mp.Lock()\n",
    "manager = mp.Manager()\n",
    "processorCores = 12\n",
    "#data_dict = manager.dict({'fenstring': [], 'data': []})\n",
    "data_dict = manager.list()\n",
    "with mp.Pool(processes=processorCores) as p:\n",
    "    args = [(fileName, src, fen_from_filename, fen_from_position, NUM_SPACES_PER_BOARD, data_dict) for fileName in trainPaths[:NUM_TRAINING_IMAGES]]\n",
    "    results = [0]\n",
    "    results = p.starmap(runt, args)\n",
    "\n",
    "# Access the shared dictionary\n",
    "print(len(data_dict))\n",
    "\n",
    "combined_results = {'fenstring': [], 'data': []}\n",
    "\n",
    "# Initialize lists to store all data and fenstring items\n",
    "all_data_items = []\n",
    "all_fenstring_items = []\n",
    "\n",
    "for result in data_dict:\n",
    "    # Extend all_data_items with result['data']\n",
    "    all_data_items.extend(result['data'])\n",
    "\n",
    "    # Extend all_fenstring_items with result['fenstring']\n",
    "    all_fenstring_items.extend(result['fenstring'])\n",
    "\n",
    "# Assign the combined lists to combined_results\n",
    "combined_results['data'] = all_data_items\n",
    "combined_results['fenstring'] = all_fenstring_items\n",
    "\n",
    "joblib.dump(combined_results, PICKLE_PATH)\n",
    "read_and_store_modified_chess_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# extracts the uniques spaces from a chess board images and saves them in directories of like squares\n",
    "def boardToSpaces():\n",
    "\n",
    "    # finds every unique square/piece combination in the fen string\n",
    "    def getUniqueImageLocations(filename):\n",
    "        #fenString = fen_from_filename(\"1b1B1b2-2pK2q1-4p1rB-7k-8-8-3B4-3rb3.jpeg\")\n",
    "        fenString = fen_from_filename(filename)\n",
    "\n",
    "        # turn the fen string into arrays in the board shape\n",
    "        stringPerRow = []\n",
    "        for row in range(8):\n",
    "            rowBeggining = row * 8\n",
    "            rowEnding = row * 8 + 8\n",
    "            stringPerRow.append(fenString[rowBeggining:rowEnding])\n",
    "\n",
    "        uniqueImages = {\n",
    "            # empty light and dark squares\n",
    "            \" l\": False,\n",
    "            \" d\": False,\n",
    "            # white pieces on dark square\n",
    "            \"WPd\": False,\n",
    "            \"WRd\": False,\n",
    "            \"WNd\": False,\n",
    "            \"WBd\": False,\n",
    "            \"WQd\": False,\n",
    "            \"WKd\": False,\n",
    "            # white pieces on light square\n",
    "            \"WPl\": False,\n",
    "            \"WRl\": False,\n",
    "            \"WNl\": False,\n",
    "            \"WBl\": False,\n",
    "            \"WQl\": False,\n",
    "            \"WKl\": False,\n",
    "            # black pieces on dark square\n",
    "            \"Bpd\": False,\n",
    "            \"Brd\": False,\n",
    "            \"Bnd\": False,\n",
    "            \"Bbd\": False,\n",
    "            \"Bqd\": False,\n",
    "            \"Bkd\": False,\n",
    "            # black pieces on light square\n",
    "            \"Bpl\": False,\n",
    "            \"Brl\": False,\n",
    "            \"Bnl\": False,\n",
    "            \"Bbl\": False,\n",
    "            \"Bql\": False,\n",
    "            \"Bkl\": False\n",
    "        }\n",
    "\n",
    "        # fills location of unique combinations to uniqueImages\n",
    "        for row in range(8):\n",
    "            for column in range(8):\n",
    "                contents = spaces[stringPerRow[row][column]] + spaceColor[row][column]\n",
    "                if not uniqueImages[contents]:\n",
    "                    #print(\"adding \", row, \", \", column, \"to \", contents)\n",
    "                    uniqueImages[contents] = str(row) + str(column)\n",
    "\n",
    "        return uniqueImages\n",
    "\n",
    "    inputFilePath = \"../chess boards/\"\n",
    "    outputFilePathBase = \"../chess boards/individualTest/\"\n",
    "\n",
    "    fileNameCounter = {\n",
    "        \"Bp\": 0,\n",
    "        \"WP\": 0,\n",
    "        \"Br\": 0,\n",
    "        \"WR\": 0,\n",
    "        \"Bn\": 0,\n",
    "        \"WN\": 0,\n",
    "        \"Bb\": 0,\n",
    "        \"WB\": 0,\n",
    "        \"Bq\": 0,\n",
    "        \"WQ\": 0,\n",
    "        \"Bk\": 0,\n",
    "        \"WK\": 0,\n",
    "        \" \": 0\n",
    "    }\n",
    "    spaceColor = [\"ldldldld\", \"dldldldl\", \"ldldldld\", \"dldldldl\", \"ldldldld\", \"dldldldl\", \"ldldldld\", \"dldldldl\"]\n",
    "    spaces = {\n",
    "        \"P\": \"WP\",\n",
    "        \"p\": \"Bp\",\n",
    "        \"R\": \"WR\",\n",
    "        \"r\": \"Br\",\n",
    "        \"N\": \"WN\",\n",
    "        \"n\": \"Bn\",\n",
    "        \"B\": \"WB\",\n",
    "        \"b\": \"Bb\",\n",
    "        \"Q\": \"WQ\",\n",
    "        \"q\": \"Bq\",\n",
    "        \"K\": \"WK\",\n",
    "        \"k\": \"Bk\",\n",
    "        \" \": \" \"\n",
    "        }\n",
    "\n",
    "    filesRead = 0\n",
    "    for inputFileName in testPaths:\n",
    "\n",
    "        print(\"on file:\", filesRead)\n",
    "        filesRead += 1\n",
    "        #inputFileName = trainPaths[index]\n",
    "        uniqueLocations = getUniqueImageLocations(inputFileName[5:])\n",
    "        image = imread(os.path.join(inputFilePath, inputFileName))\n",
    "        #scaledImage = rescale(image, .5, channel_axis = -1)\n",
    "        #grayImage = color.rgb2gray(scaledImage)\n",
    "        grayImage = color.rgb2gray(image)\n",
    "        scaledGray = rescale(grayImage, .5)\n",
    "        normalizedGray = (scaledGray * 255).astype(np.uint8)\n",
    "\n",
    "        #print(\"Input filename:\", inputFileName)\n",
    "\n",
    "        for key in uniqueLocations:\n",
    "            if uniqueLocations[key]:\n",
    "                if key[0] == \" \":\n",
    "                    outputFilePath = outputFilePathBase + \"e\" + \"/\"\n",
    "                    outputFileName = \"e\" + \"_\" + str(fileNameCounter[key[0]]) + \".jpeg\"\n",
    "                    fileNameCounter[key[0]] += 1\n",
    "                else:\n",
    "                    outputFilePath = outputFilePathBase + key[:2] + \"/\"\n",
    "                    outputFileName = key[:2] + \"_\" + str(fileNameCounter[key[:2]]) + \".jpeg\"\n",
    "                    fileNameCounter[key[:2]] += 1\n",
    "\n",
    "                #print(\"outputFileName: \", outputFileName)\n",
    "                yLoc = 25 * int(uniqueLocations[key][0])\n",
    "                xLoc = 25 * int(uniqueLocations[key][1])\n",
    "                outputImage = normalizedGray[yLoc : yLoc + 25, xLoc : xLoc + 25]\n",
    "                imsave(os.path.join(outputFilePath, outputFileName), outputImage, check_contrast=False)\n",
    "\n",
    "convertBoards = False\n",
    "if convertBoards:\n",
    "    boardToSpaces()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training dataset...\n",
      "Found 842149 files belonging to 13 classes.\n",
      "Training dataset Created.\n",
      "Saving training dataset...\n",
      "Training dataset saved.\n",
      "Creating testing dataset...\n",
      "Found 842149 files belonging to 13 classes.\n",
      "Testing dataset Created.\n",
      "Saving Testing dataset...\n",
      "Testing dataset saved.\n",
      "classWeights: {0: 2.7942717429269996, 1: 2.0, 2: 2.8133352088901393, 3: 2.770850651149903, 4: 5.05050505050505, 5: 2.785515320334262, 6: 1.0, 7: 2.8083971073509795, 8: 2.0, 9: 2.811160306416473, 10: 2.764531066417859, 11: 5.051780752715332, 12: 2.848597065945022}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\"\"\"\n",
    "0 = all messages are logged (default behavior)\n",
    "1 = INFO messages are not printed\n",
    "2 = INFO and WARNING messages are not printed\n",
    "3 = INFO, WARNING, and ERROR messages are not printed \"\"\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from keras import callbacks\n",
    "\n",
    "# wsl file path\n",
    "#dataset_url = \"file:///mnt/c/Users/kevdi/Desktop/UA Files/Applied Machine Learning/Project 3/chess boards.zip\"\n",
    "datasetDirectoryPath = \"../chess boards/individualTrain\"\n",
    "classname = [\"BB\",\n",
    "\"BK\",\n",
    "\"BN\",\n",
    "\"BP\",\n",
    "\"BQ\",\n",
    "\"BR\",\n",
    "\"e\",\n",
    "\"WB\",\n",
    "\"WK\",\n",
    "\"WN\",\n",
    "\"WP\",\n",
    "\"WQ\",\n",
    "\"WR\"]\n",
    "\n",
    "try:\n",
    "    train_images_ds = tf.data.Dataset.load('./dataSets/traindata')\n",
    "    print(\"Train Dataset loaded.\\n\")\n",
    "except:\n",
    "    print(\"Creating training dataset...\")\n",
    "    train_images_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        datasetDirectoryPath,\n",
    "        seed = 1337,\n",
    "        image_size = (25, 25), # resizes images to specified size\n",
    "        shuffle = True, # sorted alphanumerically otherwise\n",
    "        batch_size=32,\n",
    "        label_mode = \"categorical\",\n",
    "        labels = \"inferred\",\n",
    "        class_names = classname,\n",
    "        color_mode = \"grayscale\",\n",
    "    )\n",
    "    print(\"Training dataset Created.\")\n",
    "\n",
    "    print(\"Saving training dataset...\")\n",
    "    tf.data.Dataset.save(train_images_ds, './dataSets/traindata')\n",
    "    print(\"Training dataset saved.\")\n",
    "\n",
    "try:\n",
    "    test_images_ds = tf.data.Dataset.load('./dataSets/testdata')\n",
    "    print(\"Test Dataset loaded.\\n\")\n",
    "except:\n",
    "    print(\"Creating testing dataset...\")\n",
    "    train_images_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        datasetDirectoryPath,\n",
    "        seed = 1337,\n",
    "        image_size = (25, 25), # resizes images to specified size\n",
    "        shuffle = True, # sorted alphanumerically otherwise\n",
    "        batch_size=32,\n",
    "        label_mode = \"categorical\",\n",
    "        labels = \"inferred\",\n",
    "        class_names = classname,\n",
    "        color_mode = \"grayscale\",\n",
    "    )\n",
    "    print(\"Testing dataset Created.\")\n",
    "\n",
    "    print(\"Saving Testing dataset...\")\n",
    "    tf.data.Dataset.save(train_images_ds, './dataSets/testdata')\n",
    "    print(\"Testing dataset saved.\")\n",
    "\n",
    "def get_weight_dict(dir):\n",
    "    most_samples=0\n",
    "    class_weight={}\n",
    "    class_list=os.listdir(dir) # dir is the directory with the training samples organized by class\n",
    "    for c in (class_list): # iterate through class directories, find number of samples in each class then find class with highest number of samples\n",
    "        c_path=os.path.join(dir,c)\n",
    "        if os.path.isdir(c_path):\n",
    "            length=len(os.listdir(c_path)) # determine number of samples in the class directory\n",
    "            if length>most_samples:\n",
    "                most_samples=length\n",
    "    for i,c in enumerate(class_list): #iterate through class directories, find number of samples in each and divide total_samples by length\n",
    "        c_path=os.path.join(dir,c)\n",
    "        if os.path.isdir(c_path):\n",
    "            length=len(os.listdir(c_path)) # number of samples inclass directory\n",
    "            class_weight[i]=most_samples/length\n",
    "            #print (i,most_samples, class_weight[i])\n",
    "    return class_weight\n",
    "classWeights = get_weight_dict(\"../chess boards/individualTest\")\n",
    "#print(\"classWeights:\", classWeights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n",
      "\n",
      "Evaluating Model..\n",
      "\u001b[1m26318/26318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m546s\u001b[0m 21ms/step - accuracy: 0.1908 - loss: 7.0088\n",
      "\n",
      "Test accuracy: 0.18999013304710388\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras import callbacks\n",
    "try:\n",
    "    model = tf.keras.models.load_model('./ChessModelScaledFinal.keras')\n",
    "    print(\"Model loaded.\\n\")\n",
    "except:\n",
    "    print(\"No saved model found.\\n\")\n",
    "    model = False\n",
    "\n",
    "runTrain = False\n",
    "\n",
    "if not model or runTrain:\n",
    "    runTrain = True\n",
    "    model = tf.keras.Sequential()\n",
    "    #model = tf.keras.Model()\n",
    "    #model.add(tf.keras.layers.Input(shape=(None,25,25,1)))\n",
    "    model.add(tf.keras.layers.Rescaling(1./255))\n",
    "    model.add(tf.keras.layers.Dense(512, activation='relu'))\n",
    "    model.add(tf.keras.layers.Conv2D(filters=128, kernel_size=3, activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D())\n",
    "    model.add(tf.keras.layers.Conv2D(filters=128, kernel_size=3, activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D())\n",
    "    model.add(tf.keras.layers.Conv2D(filters=128, kernel_size=3, activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D())\n",
    "\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(512, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(13))\n",
    "    model.compile(optimizer='adam',\n",
    "                loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    earlystopping = callbacks.EarlyStopping(monitor=\"loss\", mode=\"min\", patience=10, restore_best_weights=True, verbose=1)\n",
    "\n",
    "    #history = model.fit(test_images_ds, epochs=1000, callbacks=[earlystopping], steps_per_epoch=1500, class_weight=classWeights)\n",
    "    history = model.fit(train_images_ds, epochs=2, callbacks=[earlystopping], class_weight=classWeights)\n",
    "\n",
    "print(\"Evaluating Model..\")\n",
    "test_loss, test_acc = model.evaluate(train_images_ds)\n",
    "\n",
    "print('\\nTest accuracy:', test_acc)\n",
    "if runTrain:\n",
    "    print(\"Saving model...\")\n",
    "    model.save('./ChessModelScaledFinal.keras')\n",
    "    print(\"Model saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
